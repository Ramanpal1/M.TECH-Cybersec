{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea2646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Section\n",
    "import sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a71a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data=load_breast_cancer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e94acde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine Data\n",
    "x_names=data['feature_names']\n",
    "y_names=data['target_names']\n",
    "x=data['data']\n",
    "y=data['target']\n",
    "y_names\n",
    "x_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01ccb72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.2,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fae1ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp=MLPClassifier(hidden_layer_sizes=(200,),activation='logistic',alpha=0.01,solver='sgd',random_state=1,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "544ded4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65923655\n",
      "Iteration 2, loss = 0.62010309\n",
      "Iteration 3, loss = 0.60089147\n",
      "Iteration 4, loss = 0.59455281\n",
      "Iteration 5, loss = 0.59059195\n",
      "Iteration 6, loss = 0.57600209\n",
      "Iteration 7, loss = 0.55934649\n",
      "Iteration 8, loss = 0.54103609\n",
      "Iteration 9, loss = 0.53169732\n",
      "Iteration 10, loss = 0.52644031\n",
      "Iteration 11, loss = 0.51491648\n",
      "Iteration 12, loss = 0.50957041\n",
      "Iteration 13, loss = 0.49531927\n",
      "Iteration 14, loss = 0.48685532\n",
      "Iteration 15, loss = 0.48186961\n",
      "Iteration 16, loss = 0.47335368\n",
      "Iteration 17, loss = 0.47236979\n",
      "Iteration 18, loss = 0.46417749\n",
      "Iteration 19, loss = 0.45020531\n",
      "Iteration 20, loss = 0.44551325\n",
      "Iteration 21, loss = 0.43861527\n",
      "Iteration 22, loss = 0.44506868\n",
      "Iteration 23, loss = 0.43381299\n",
      "Iteration 24, loss = 0.42320364\n",
      "Iteration 25, loss = 0.41955603\n",
      "Iteration 26, loss = 0.41446070\n",
      "Iteration 27, loss = 0.40791881\n",
      "Iteration 28, loss = 0.40330720\n",
      "Iteration 29, loss = 0.39793433\n",
      "Iteration 30, loss = 0.40334355\n",
      "Iteration 31, loss = 0.39010784\n",
      "Iteration 32, loss = 0.39003272\n",
      "Iteration 33, loss = 0.38635656\n",
      "Iteration 34, loss = 0.38564583\n",
      "Iteration 35, loss = 0.37817154\n",
      "Iteration 36, loss = 0.37499014\n",
      "Iteration 37, loss = 0.37281393\n",
      "Iteration 38, loss = 0.36676378\n",
      "Iteration 39, loss = 0.36106657\n",
      "Iteration 40, loss = 0.35494316\n",
      "Iteration 41, loss = 0.39566343\n",
      "Iteration 42, loss = 0.35390467\n",
      "Iteration 43, loss = 0.34703682\n",
      "Iteration 44, loss = 0.34024369\n",
      "Iteration 45, loss = 0.34056843\n",
      "Iteration 46, loss = 0.33872109\n",
      "Iteration 47, loss = 0.33076108\n",
      "Iteration 48, loss = 0.33019263\n",
      "Iteration 49, loss = 0.32468236\n",
      "Iteration 50, loss = 0.32489669\n",
      "Iteration 51, loss = 0.32856873\n",
      "Iteration 52, loss = 0.32935133\n",
      "Iteration 53, loss = 0.31908653\n",
      "Iteration 54, loss = 0.31247530\n",
      "Iteration 55, loss = 0.30771844\n",
      "Iteration 56, loss = 0.31260855\n",
      "Iteration 57, loss = 0.34379320\n",
      "Iteration 58, loss = 0.30503679\n",
      "Iteration 59, loss = 0.29869738\n",
      "Iteration 60, loss = 0.30375063\n",
      "Iteration 61, loss = 0.30349112\n",
      "Iteration 62, loss = 0.31429799\n",
      "Iteration 63, loss = 0.29181528\n",
      "Iteration 64, loss = 0.29694318\n",
      "Iteration 65, loss = 0.28899344\n",
      "Iteration 66, loss = 0.30068137\n",
      "Iteration 67, loss = 0.30659270\n",
      "Iteration 68, loss = 0.29054381\n",
      "Iteration 69, loss = 0.28377394\n",
      "Iteration 70, loss = 0.28061753\n",
      "Iteration 71, loss = 0.27845897\n",
      "Iteration 72, loss = 0.28875502\n",
      "Iteration 73, loss = 0.28133323\n",
      "Iteration 74, loss = 0.27569054\n",
      "Iteration 75, loss = 0.27613507\n",
      "Iteration 76, loss = 0.26998642\n",
      "Iteration 77, loss = 0.26849829\n",
      "Iteration 78, loss = 0.27507900\n",
      "Iteration 79, loss = 0.27318507\n",
      "Iteration 80, loss = 0.26653420\n",
      "Iteration 81, loss = 0.26529574\n",
      "Iteration 82, loss = 0.26597377\n",
      "Iteration 83, loss = 0.26832073\n",
      "Iteration 84, loss = 0.26832749\n",
      "Iteration 85, loss = 0.26264180\n",
      "Iteration 86, loss = 0.29211899\n",
      "Iteration 87, loss = 0.26972070\n",
      "Iteration 88, loss = 0.26124743\n",
      "Iteration 89, loss = 0.26146951\n",
      "Iteration 90, loss = 0.25928836\n",
      "Iteration 91, loss = 0.26549789\n",
      "Iteration 92, loss = 0.25724246\n",
      "Iteration 93, loss = 0.25902636\n",
      "Iteration 94, loss = 0.25207262\n",
      "Iteration 95, loss = 0.27310921\n",
      "Iteration 96, loss = 0.28992534\n",
      "Iteration 97, loss = 0.25531794\n",
      "Iteration 98, loss = 0.24941219\n",
      "Iteration 99, loss = 0.24725687\n",
      "Iteration 100, loss = 0.24738038\n",
      "Iteration 101, loss = 0.28736272\n",
      "Iteration 102, loss = 0.25478691\n",
      "Iteration 103, loss = 0.25266879\n",
      "Iteration 104, loss = 0.27009862\n",
      "Iteration 105, loss = 0.26271410\n",
      "Iteration 106, loss = 0.24379785\n",
      "Iteration 107, loss = 0.25103122\n",
      "Iteration 108, loss = 0.25200575\n",
      "Iteration 109, loss = 0.27334823\n",
      "Iteration 110, loss = 0.26555587\n",
      "Iteration 111, loss = 0.24605831\n",
      "Iteration 112, loss = 0.26522007\n",
      "Iteration 113, loss = 0.24223532\n",
      "Iteration 114, loss = 0.25933773\n",
      "Iteration 115, loss = 0.24595163\n",
      "Iteration 116, loss = 0.24144297\n",
      "Iteration 117, loss = 0.25221602\n",
      "Iteration 118, loss = 0.25327025\n",
      "Iteration 119, loss = 0.24088654\n",
      "Iteration 120, loss = 0.24677844\n",
      "Iteration 121, loss = 0.24499801\n",
      "Iteration 122, loss = 0.23618423\n",
      "Iteration 123, loss = 0.23436268\n",
      "Iteration 124, loss = 0.23850980\n",
      "Iteration 125, loss = 0.23534783\n",
      "Iteration 126, loss = 0.23145337\n",
      "Iteration 127, loss = 0.24282401\n",
      "Iteration 128, loss = 0.24280354\n",
      "Iteration 129, loss = 0.23635408\n",
      "Iteration 130, loss = 0.24119333\n",
      "Iteration 131, loss = 0.23738306\n",
      "Iteration 132, loss = 0.23851356\n",
      "Iteration 133, loss = 0.23641084\n",
      "Iteration 134, loss = 0.23005583\n",
      "Iteration 135, loss = 0.24338351\n",
      "Iteration 136, loss = 0.23231458\n",
      "Iteration 137, loss = 0.24372312\n",
      "Iteration 138, loss = 0.23345350\n",
      "Iteration 139, loss = 0.24349666\n",
      "Iteration 140, loss = 0.23158417\n",
      "Iteration 141, loss = 0.23671635\n",
      "Iteration 142, loss = 0.23149554\n",
      "Iteration 143, loss = 0.23861981\n",
      "Iteration 144, loss = 0.22897235\n",
      "Iteration 145, loss = 0.23123335\n",
      "Iteration 146, loss = 0.22731806\n",
      "Iteration 147, loss = 0.23948974\n",
      "Iteration 148, loss = 0.23630471\n",
      "Iteration 149, loss = 0.23025184\n",
      "Iteration 150, loss = 0.23720603\n",
      "Iteration 151, loss = 0.23479344\n",
      "Iteration 152, loss = 0.22744622\n",
      "Iteration 153, loss = 0.23030975\n",
      "Iteration 154, loss = 0.25253330\n",
      "Iteration 155, loss = 0.25619034\n",
      "Iteration 156, loss = 0.23374004\n",
      "Iteration 157, loss = 0.24094016\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, alpha=0.01, hidden_layer_sizes=(200,),\n",
       "              random_state=1, solver=&#x27;sgd&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, alpha=0.01, hidden_layer_sizes=(200,),\n",
       "              random_state=1, solver=&#x27;sgd&#x27;, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.01, hidden_layer_sizes=(200,),\n",
       "              random_state=1, solver='sgd', verbose=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53a682bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction= mlp.predict(x_test)\n",
    "metrics.accuracy_score(y_test,prediction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93416a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
